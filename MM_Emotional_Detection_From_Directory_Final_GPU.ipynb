{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOAzeemKhan/MM_Reaction_Analysis/blob/main/MM_Emotional_Detection_From_Directory_Final_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOi7nfzPqFTW"
      },
      "source": [
        "# Approach 1\n",
        "## About Model: ViT-Face-Expression model from Hugging Face, a transformer-based pre-trained model explicitly designed for emotion detection tasks.\n",
        "\n",
        "## Using: Facenet_pytorch's MTCNN for accurate face detection and transformers for leveraging the cutting-edge ViT-Face-Expression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMD7-LUUrnE8"
      },
      "outputs": [],
      "source": [
        "!pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alVpqtoGp76j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from moviepy.editor import VideoFileClip, ImageSequenceClip\n",
        "\n",
        "import torch\n",
        "from facenet_pytorch import (MTCNN)\n",
        "\n",
        "from transformers import (AutoFeatureExtractor,\n",
        "                          AutoModelForImageClassification,\n",
        "                          AutoConfig)\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from moviepy.editor import VideoFileClip\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "import time\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sspSxHDusCAW"
      },
      "outputs": [],
      "source": [
        "def detect_emotions(image):\n",
        "    print(\"In Detect Emotions\")\n",
        "    \"\"\"\n",
        "    Detect emotions from a given image, displays the detected\n",
        "    face and the emotion probabilities in a bar plot.\n",
        "\n",
        "    Parameters:\n",
        "    image (PIL.Image): The input image.\n",
        "\n",
        "    Returns:\n",
        "    PIL.Image: The cropped face from the input image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a copy of the image to draw on\n",
        "    temporary = image.copy()\n",
        "    #plt.imshow(temporary)\n",
        "    # Use the MTCNN model to detect faces in the image\n",
        "    sample = mtcnn.detect(temporary)\n",
        "    print(sample)\n",
        "    # If a face is detected\n",
        "    if sample[0] is not None:\n",
        "        #print(\"IN IFFFF\")\n",
        "        # Get the bounding box coordinates of the face\n",
        "        box = sample[0][0]\n",
        "\n",
        "        # Crop the detected face from the image\n",
        "        face = temporary.crop(box)\n",
        "\n",
        "        # Pre-process the cropped face to be fed into the\n",
        "        # emotion detection model\n",
        "        inputs = extractor(images=face, return_tensors=\"pt\")\n",
        "\n",
        "        # Pass the pre-processed face through the model to\n",
        "        # get emotion predictions\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Apply softmax to the logits to get probabilities\n",
        "        probabilities = torch.nn.functional.softmax(outputs.logits,\n",
        "                                                    dim=-1)\n",
        "\n",
        "        # Retrieve the id2label attribute from the configuration\n",
        "        id2label = AutoConfig.from_pretrained(\n",
        "            \"trpakov/vit-face-expression\", cache_dir = path_cache\n",
        "        ).id2label\n",
        "\n",
        "        # Convert probabilities tensor to a Python list\n",
        "        probabilities = probabilities.detach().numpy().tolist()[0]\n",
        "\n",
        "        # Map class labels to their probabilities\n",
        "        class_probabilities = {id2label[i]: prob for i,\n",
        "                               prob in enumerate(probabilities)}\n",
        "\n",
        "        # Define colors for each emotion\n",
        "        colors = {\n",
        "            \"angry\": \"red\",\n",
        "            \"disgust\": \"green\",\n",
        "            \"fear\": \"gray\",\n",
        "            \"happy\": \"yellow\",\n",
        "            \"neutral\": \"purple\",\n",
        "            \"sad\": \"blue\",\n",
        "            \"surprise\": \"orange\"\n",
        "        }\n",
        "        #palette = [colors[label] for label in class_probabilities.keys()]\n",
        "        #print(\"IN IFFF\")\n",
        "        # Prepare a figure with 2 subplots: one for the face image,\n",
        "        # one for the bar plot\n",
        "        #fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Display the cropped face in the left subplot\n",
        "        #axs[0].imshow(np.array(face))\n",
        "        #axs[0].axis('off')\n",
        "\n",
        "        # Create a horizontal bar plot of the emotion probabilities in\n",
        "        # the right subplot\n",
        "        #sns.barplot(ax=axs[1],\n",
        "                    #y=list(class_probabilities.keys()),\n",
        "                    #x=[prob * 100 for prob in class_probabilities.values()],\n",
        "                    #palette=palette,\n",
        "                    #orient='h')\n",
        "        #axs[1].set_xlabel('Probability (%)')\n",
        "        #axs[1].set_title('Emotion Probabilities')\n",
        "        #axs[1].set_xlim([0, 100])  # Set x-axis limits to show percentages\n",
        "\n",
        "        # Show the plot\n",
        "        #plt.show()\n",
        "        del inputs, outputs, probabilities\n",
        "        torch.cuda.empty_cache()\n",
        "        return face, class_probabilities\n",
        "    del temporary, sample\n",
        "    gc.collect()\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoCt6WQ-tAuv"
      },
      "outputs": [],
      "source": [
        "def create_combined_image(face, class_probabilities):\n",
        "    print(\"In Create Combined Functions\")\n",
        "    \"\"\"\n",
        "    Create an image combining the detected face and a barplot\n",
        "    of the emotion probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    face (PIL.Image): The detected face.\n",
        "    class_probabilities (dict): The probabilities of each\n",
        "        emotion class.\n",
        "\n",
        "    Returns:\n",
        "    np.array: The combined image as a numpy array.\n",
        "    \"\"\"\n",
        "    # Define colors for each emotion\n",
        "    colors = {\n",
        "        \"angry\": \"red\",\n",
        "        \"disgust\": \"green\",\n",
        "        \"fear\": \"gray\",\n",
        "        \"happy\": \"yellow\",\n",
        "        \"neutral\": \"purple\",\n",
        "        \"sad\": \"blue\",\n",
        "        \"surprise\": \"orange\"\n",
        "    }\n",
        "    palette = [colors[label] for label in class_probabilities.keys()]\n",
        "\n",
        "    # Create a figure with 2 subplots: one for the\n",
        "    # face image, one for the barplot\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Display face on the left subplot\n",
        "    axs[0].imshow(np.array(face))\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # Create a barplot of the emotion probabilities\n",
        "    # on the right subplot\n",
        "    sns.barplot(ax=axs[1],\n",
        "                y=list(class_probabilities.keys()),\n",
        "                x=[prob * 100 for prob in class_probabilities.values()],\n",
        "                palette=palette,\n",
        "                orient='h')\n",
        "    axs[1].set_xlabel('Probability (%)')\n",
        "    axs[1].set_title('Emotion Probabilities')\n",
        "    axs[1].set_xlim([0, 100])  # Set x-axis limits\n",
        "\n",
        "    # Convert the figure to a numpy array\n",
        "    canvas = FigureCanvas(fig)\n",
        "    canvas.draw()\n",
        "    img = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
        "    img  = img.reshape(canvas.get_width_height()[::-1] + (3,))\n",
        "\n",
        "    plt.close(fig)\n",
        "    del fig, axs, canvas, palette\n",
        "    gc.collect()\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_cache = \"/content/\"\n",
        "# Set cache directories for XDG and Hugging Face Hub\n",
        "os.environ['XDG_CACHE_HOME'] = '/home/msds2023/jlegara/.cache'\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = '/home/msds2023/jlegara/.cache'"
      ],
      "metadata": {
        "id": "unHXCM4K8he7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cache directories for XDG and Hugging Face Hub\n",
        "#os.environ['XDG_CACHE_HOME'] = '/data/mpstme-azeem/msds2023/jlegara/.cache'\n",
        "#os.environ['HUGGINGFACE_HUB_CACHE'] = '/data/mpstme-azeem/msds2023/jlegara/.cache'"
      ],
      "metadata": {
        "id": "fGarYp6R8mkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHGakH7dsFTf"
      },
      "outputs": [],
      "source": [
        "# Initialize MTCNN model for single face cropping\n",
        "mtcnn = MTCNN(\n",
        "    image_size=160,\n",
        "    margin=0,\n",
        "    min_face_size=200,\n",
        "    thresholds=[0.6, 0.7, 0.7],\n",
        "    factor=0.709,\n",
        "    post_process=True,\n",
        "    keep_all=False,\n",
        "    device=device\n",
        ")\n",
        "# Load the pre-trained model and feature extractor\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    \"trpakov/vit-face-expression\", cache_dir = path_cache\n",
        ")\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"trpakov/vit-face-expression\", cache_dir = path_cache\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-7iP0OLOLL_"
      },
      "outputs": [],
      "source": [
        "# Set cache directories for XDG and Hugging Face Hub\n",
        "#os.environ['XDG_CACHE_HOME'] = '/data/mpstme-azeem/msds2023/jlegara/.cache'\n",
        "#os.environ['HUGGINGFACE_HUB_CACHE'] = '/data/mpstme-azeem/msds2023/jlegara/.cache'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cache directories for XDG and Hugging Face Hub\n",
        "os.environ['XDG_CACHE_HOME'] = '/home/msds2023/jlegara/.cache'\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = '/home/msds2023/jlegara/.cache'"
      ],
      "metadata": {
        "id": "Sdg3GKc5Pnj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_video_into_chunks(path, output_dir):\n",
        "    start2 = time.time()\n",
        "    print(\"In Split Video into Chunks Function\")\n",
        "\n",
        "    # Load your video\n",
        "    clip = VideoFileClip(path)\n",
        "    vid_fps = clip.fps\n",
        "    video = clip.without_audio()\n",
        "\n",
        "    batch_size = 32\n",
        "    chunk_size = 500\n",
        "    batch_frames = []\n",
        "    current_timestamps = []\n",
        "    chunk_count = 0\n",
        "\n",
        "    # Initialize current_chunk and current_chunk_timestamps\n",
        "    current_chunk = []\n",
        "    current_chunk_timestamps = []\n",
        "\n",
        "    # Process video frames\n",
        "    for t, frame in tqdm(video.iter_frames(with_times=True)):\n",
        "        batch_frames.append(np.array(frame))\n",
        "        current_timestamps.append(t)\n",
        "\n",
        "        if len(batch_frames) >= batch_size:\n",
        "            current_chunk.extend(batch_frames)\n",
        "            current_chunk_timestamps.extend(current_timestamps)\n",
        "            current_timestamps = []\n",
        "            batch_frames = []\n",
        "\n",
        "        if len(current_chunk) >= chunk_size:\n",
        "            # Save current chunk\n",
        "            np.save(os.path.join(output_dir, f'video_chunk_{chunk_count}.npy'), np.array(current_chunk))\n",
        "            np.save(os.path.join(output_dir, f'timestamps_chunk_{chunk_count}.npy'), np.array(current_chunk_timestamps))\n",
        "            chunk_count += 1\n",
        "            # Clear current chunk\n",
        "            current_chunk = []\n",
        "            current_chunk_timestamps = []\n",
        "            gc.collect()\n",
        "\n",
        "    # Save any remaining frames\n",
        "    if len(current_chunk) > 0:\n",
        "        np.save(os.path.join(output_dir, f'video_chunk_{chunk_count}.npy'), np.array(current_chunk))\n",
        "        np.save(os.path.join(output_dir, f'timestamps_chunk_{chunk_count}.npy'), np.array(current_chunk_timestamps))\n",
        "\n",
        "    # Close the video clip\n",
        "    clip.reader.close()\n",
        "    if clip.audio:\n",
        "        clip.audio.reader.close_proc()\n",
        "\n",
        "    # Delete variables and collect garbage\n",
        "    del video, batch_frames, current_chunk, current_timestamps, current_chunk_timestamps, clip\n",
        "    gc.collect()\n",
        "\n",
        "    end2 = time.time()\n",
        "    print(f\"Total chunks saved: {chunk_count + 1}\\nTotal Chunking Took: {end2 - start2}\")\n",
        "    return vid_fps\n"
      ],
      "metadata": {
        "id": "BWuuCtKmle05"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rest_code(path_chunk, chunk_output_dir, vid_fps, skips):\n",
        "  print(\"In Rest Code Function\")\n",
        "\n",
        "  #chunk_dir -> path of a particular chunk\n",
        "  #path_chunk -> path of the directory containing all chunks\n",
        "  # Take care of timestamp\n",
        "\n",
        "  # Ensure the output directory exists\n",
        "  os.makedirs(chunk_output_dir, exist_ok=True)\n",
        "\n",
        "  npy_files = [f for f in os.listdir(path_chunk) if f.endswith('.npy') and 'video_chunk' in f]\n",
        "\n",
        "  for file in npy_files:\n",
        "    reduced_video = []\n",
        "    # List to hold the combined images\n",
        "    combined_images = []\n",
        "    # Create a list to hold the class probabilities for all frames\n",
        "    all_class_probabilities = []\n",
        "\n",
        "    print(f\"Processing chunk: {file}\")\n",
        "\n",
        "    chunk_dir = os.path.join(path_chunk, file)\n",
        "    loaded_chunk = np.load(chunk_dir)\n",
        "    timestamp_chunk = np.load(chunk_dir.replace('video_chunk', 'timestamps_chunk'))\n",
        "\n",
        "    print(f\"Loaded chunk shape: {loaded_chunk.shape}\")\n",
        "\n",
        "    for i in tqdm(range(0, len(loaded_chunk), skips)):\n",
        "        reduced_video.append(loaded_chunk[i])\n",
        "\n",
        "    reduced_timestamps = [timestamp_chunk[i] for i in range(0, len(timestamp_chunk), skips)]\n",
        "\n",
        "    # Define a list of emotions\n",
        "    emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "\n",
        "    #timestamps = []\n",
        "\n",
        "    # Loop over video frames\n",
        "    for i, frame in tqdm(enumerate(reduced_video),\n",
        "                        total=len(reduced_video),\n",
        "                        desc=\"Processing frames\"):\n",
        "        # Convert frame to uint8\n",
        "        frame = frame.astype(np.uint8)\n",
        "        #print(frame)\n",
        "        # Calculate the accurate timestamp for this frame\n",
        "        # Since we're skipping frames, the timestamp is calculated based on the original frame index (i * skips)\n",
        "        #timestamp = (i * skips) * time_per_frame\n",
        "        #timestamps.append(timestamp)\n",
        "\n",
        "        # Call detect_emotions to get face and class probabilities\n",
        "        #print(Image.fromarray(frame))\n",
        "        face, class_probabilities = detect_emotions(Image.fromarray(frame))\n",
        "\n",
        "        # If a face was found\n",
        "        if face is not None:\n",
        "            # Create combined image for this frame\n",
        "            combined_image = create_combined_image(face, class_probabilities)\n",
        "\n",
        "            # Append combined image to the list\n",
        "            combined_images.append(combined_image)\n",
        "        else:\n",
        "            # If no face was found, set class probabilities to None\n",
        "            class_probabilities = {emotion: None for emotion in emotions}\n",
        "\n",
        "        # Append class probabilities to the list\n",
        "        all_class_probabilities.append(class_probabilities)\n",
        "\n",
        "    # Convert list of images to video clip\n",
        "    clip_with_plot = ImageSequenceClip(combined_images,\n",
        "                                      fps=vid_fps/skips)  # Choose the frame rate (fps) according to your requirement\n",
        "\n",
        "    # Generate unique filenames for this chunk\n",
        "    chunk_name = os.path.basename(chunk_dir).split('.')[0]\n",
        "    video_output_path = os.path.join(chunk_output_dir, f\"{chunk_name}_plot_video.mp4\")\n",
        "    #prob_output_path = os.path.join(chunk_output_dir, f\"{chunk_name}_class_probabilities.pkl\")\n",
        "    df_output_path = os.path.join(chunk_output_dir, f\"{chunk_name}_class_probabilities.csv\")\n",
        "    #reduced_video_output_path = os.path.join(chunk_output_dir, f\"{chunk_name}_reduced_video.npy\")\n",
        "\n",
        "    # Write the video to a file with a specific frame rate\n",
        "    clip_with_plot.write_videofile(video_output_path, fps=vid_fps/skips)\n",
        "\n",
        "    # Define colors for each emotion\n",
        "    colors = {\n",
        "        \"angry\": \"red\",\n",
        "        \"disgust\": \"green\",\n",
        "        \"fear\": \"gray\",\n",
        "        \"happy\": \"yellow\",\n",
        "        \"neutral\": \"purple\",\n",
        "        \"sad\": \"blue\",\n",
        "        \"surprise\": \"orange\"\n",
        "    }\n",
        "\n",
        "    # Convert list of class probabilities into a DataFrame\n",
        "    df = pd.DataFrame(all_class_probabilities)\n",
        "\n",
        "    # Convert probabilities to percentages\n",
        "    df = df * 100\n",
        "    df['timestamp'] = reduced_timestamps\n",
        "    # Save DataFrame to CSV file\n",
        "    df.to_csv(df_output_path, index=False)\n",
        "\n",
        "    print(f\"Last timestamp: {reduced_timestamps[-1]}\")\n",
        "    print(f\"Processed chunk saved: {video_output_path},{df_output_path}\")\n",
        "\n",
        "    # Clear memory after each chunk is processed\n",
        "    del reduced_video\n",
        "    del combined_images\n",
        "    del all_class_probabilities\n",
        "    del clip_with_plot\n",
        "    del df\n",
        "\n",
        "  print(f\"All Processed Chunks Successfully saved to: {chunk_output_dir}\")"
      ],
      "metadata": {
        "id": "sejxg1qircXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(chunk_output_dir, final_dir):\n",
        "    chunk_dir = chunk_output_dir\n",
        "    print(\"In Merge Function\")\n",
        "    start = time.time()\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(final_dir, exist_ok=True)\n",
        "    os.makedirs(chunk_output_dir, exist_ok=True)\n",
        "\n",
        "    final_df_path = os.path.join(final_dir, \"final_class_probabilities.csv\")\n",
        "    output_file = os.path.join(final_dir, \"final_merged_video.mp4\")\n",
        "\n",
        "    # List all CSV files in the chunk output directory\n",
        "    csv_files = [f for f in os.listdir(chunk_output_dir) if f.endswith('_class_probabilities.csv')]\n",
        "\n",
        "    # Sort the CSV files numerically by the chunk number (extracted from 'video_chunk_<number>_class_probabilities.csv')\n",
        "    csv_files = sorted(csv_files, key=lambda x: int(x.split('_')[2]))\n",
        "\n",
        "    # Initialize an empty DataFrame to store the final merged DataFrame\n",
        "    final_df = pd.DataFrame()\n",
        "\n",
        "    # Variable to track the current frame count across all chunks\n",
        "    frame_count = 0\n",
        "\n",
        "    # Iterate over each CSV file in the correct order and concatenate them\n",
        "    for csv_file in csv_files:\n",
        "        # Load the chunk DataFrame\n",
        "        chunk_df = pd.read_csv(os.path.join(chunk_output_dir, csv_file))\n",
        "\n",
        "        # Update the index to reflect the correct frame order\n",
        "        chunk_df.index = range(frame_count, frame_count + len(chunk_df))\n",
        "\n",
        "        # Append the chunk DataFrame to the final DataFrame\n",
        "        final_df = pd.concat([final_df, chunk_df], axis=0)\n",
        "\n",
        "        # Update the frame count\n",
        "        frame_count += len(chunk_df)\n",
        "\n",
        "    final_df.to_csv(final_df_path, index=True)\n",
        "    # Check timestamp distribution\n",
        "    print(f\"Timestamp range: {final_df['timestamp'].min()} - {final_df['timestamp'].max()}\")\n",
        "    print(f\"Final DataFrame saved to: {final_df_path}\")\n",
        "\n",
        "    # Define colors for each emotion\n",
        "    colors = {\"angry\": \"red\",\"disgust\": \"green\",\"fear\": \"gray\",\"happy\": \"yellow\",\"neutral\": \"purple\",\"sad\": \"blue\",\"surprise\": \"orange\"}\n",
        "\n",
        "    #Exclude 'timestamp' from columns for plotting emotions, but use it as the x-axis\n",
        "    columns_to_plot = [col for col in final_df.columns if col != 'timestamp']\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for emotion in columns_to_plot:\n",
        "        plt.plot(final_df['timestamp'], final_df[emotion], label=emotion, color=colors[emotion])\n",
        "\n",
        "    plt.xlabel('Timestamp (seconds)')\n",
        "    plt.ylabel('Emotion Probability (%)')\n",
        "    plt.title('Emotion Probabilities Over Time')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(final_dir, \"final_plot.png\"))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # List all processed video chunks in the directory\n",
        "    video_files = [f for f in os.listdir(chunk_output_dir) if f.endswith('_plot_video.mp4')]\n",
        "\n",
        "    # Sort the files in ascending order based on chunk number\n",
        "    video_files.sort(key=lambda f: int(f.split('_')[2]))\n",
        "\n",
        "    # List to hold video clips\n",
        "    clips = []\n",
        "\n",
        "    # Load each video chunk and append to clips list\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(chunk_dir, video_file)\n",
        "        clip = VideoFileClip(video_path)\n",
        "        clips.append(clip)\n",
        "\n",
        "    # Concatenate the video clips in the correct order\n",
        "    final_clip = concatenate_videoclips(clips, method=\"compose\")\n",
        "\n",
        "    # Write the final merged video to output_file\n",
        "    final_clip.write_videofile(output_file)\n",
        "\n",
        "    # Close all the clips to release resources\n",
        "    for clip in clips:\n",
        "        clip.close()\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Merged video saved to {output_file}\")\n",
        "    print(f\"Total Merging Took: {end-start}\")"
      ],
      "metadata": {
        "id": "NECw8VC-xrsc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def working(video_dir, output_base_dir, skips):\n",
        "  print(\"In working Function\")\n",
        "  os.makedirs(output_base_dir, exist_ok=True)\n",
        "\n",
        "  start1 = time.time()\n",
        "  # Directory where chunks are saved\n",
        "  final_output_dir = os.path.join(output_base_dir, 'processed_reaction_videos')\n",
        "  os.makedirs(final_output_dir, exist_ok=True)\n",
        "\n",
        "  video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
        "  for video_file in video_files:\n",
        "\n",
        "    video_name = os.path.splitext(video_file)[0]  # Get filename without extension\n",
        "    video_output_dir = os.path.join(final_output_dir, video_name)\n",
        "    chunk_output_dir = os.path.join(video_output_dir, 'chunks')\n",
        "    processed_output_dir = os.path.join(video_output_dir, 'processed_data')\n",
        "    #final_video_dir = os.path.join(video_output_dir, 'final_video')\n",
        "\n",
        "    # Create directory to save the chunks\n",
        "    os.makedirs(chunk_output_dir, exist_ok=True)\n",
        "\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    print(f\"Processing video: {video_file}\")\n",
        "\n",
        "    vid_fps = split_video_into_chunks(video_path, chunk_output_dir)\n",
        "    print(vid_fps)\n",
        "    print(f\"Splitting in Chunks Completed, Chunks saved at: {chunk_output_dir}\")\n",
        "\n",
        "    processed_output_dir = os.path.join(video_output_dir, 'processed_frames')\n",
        "    final_dir = os.path.join(video_output_dir, 'final_video_plot')\n",
        "    os.makedirs(processed_output_dir, exist_ok=True)\n",
        "    #os.makedirs(final_video_dir, exist_ok=True)\n",
        "\n",
        "    rest_code(chunk_output_dir, processed_output_dir, vid_fps, skips)\n",
        "    merge(processed_output_dir, final_dir)\n",
        "    end1 = time.time()\n",
        "    print(f\"Total Processing Time: {end1-start1}\")"
      ],
      "metadata": {
        "id": "C7vWRQAZr_bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "working(\"/content/drive/MyDrive/MM/Code/test\", \"/content/drive/MyDrive/MM/Code/test/\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gBhV7Bb4Dxz",
        "outputId": "2a274e2d-fe75-4728-f415-f2fb1d1193a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In working Function\n",
            "Processing video: Copy of daivik_cropped_clipped.mp4\n",
            "In Split Video into Chunks Function\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "508it [00:04, 81.92it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jAm5_o-q2ND"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}