{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOAzeemKhan/MM_Reaction_Analysis/blob/main/MM_Emotional_Detection_Updated_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOi7nfzPqFTW"
      },
      "source": [
        "# Approach 1\n",
        "## About Model: ViT-Face-Expression model from Hugging Face, a transformer-based pre-trained model explicitly designed for emotion detection tasks.\n",
        "\n",
        "## Using: Facenet_pytorch's MTCNN for accurate face detection and transformers for leveraging the cutting-edge ViT-Face-Expression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bMD7-LUUrnE8",
        "outputId": "d332e285-b5db-4688-bae9-1b8e734ff887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Collecting Pillow<10.3.0,>=10.2.0 (from facenet-pytorch)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet-pytorch)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, facenet-pytorch\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.22.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.22.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.22.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.0+cu121\n",
            "    Uninstalling torchvision-0.19.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.19.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 facenet-pytorch-2.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "4d4815d1b34849cdb2958b09ac348bd8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKoJZmUJOLL9"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDr_KrtFOLL9"
      },
      "outputs": [],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktaguZ8KOLL-"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSdtuxNuOLL-"
      },
      "outputs": [],
      "source": [
        "!pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUizckfrOLL_"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/MM/Daivik Jayan M video reaction.mov\"\n",
        "# Directory to save frame chunks\n",
        "#output_dir = \"/data/mpstme-azeem/video_frames/\"\n",
        "output_dir = \"/content/video_frames/\"\n",
        "# Directory where chunks are saved\n",
        "#clip_output_dir = \"/data/mpstme-azeem/video_clips/\"\n",
        "clip_output_dir = \"/content/video_clips/\"\n",
        "#path_cache = \"/data/mpstme-azeem\"\n",
        "path_cache = \"/content/\""
      ],
      "metadata": {
        "id": "C7vWRQAZr_bL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alVpqtoGp76j",
        "outputId": "0fcaf693-1bbc-4b94-c227-879cf94c240b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from moviepy.editor import VideoFileClip, ImageSequenceClip\n",
        "\n",
        "import torch\n",
        "from facenet_pytorch import (MTCNN)\n",
        "\n",
        "from transformers import (AutoFeatureExtractor,\n",
        "                          AutoModelForImageClassification,\n",
        "                          AutoConfig)\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from moviepy.editor import VideoFileClip\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sspSxHDusCAW"
      },
      "outputs": [],
      "source": [
        "def detect_emotions(image):\n",
        "    \"\"\"\n",
        "    Detect emotions from a given image, displays the detected\n",
        "    face and the emotion probabilities in a bar plot.\n",
        "\n",
        "    Parameters:\n",
        "    image (PIL.Image): The input image.\n",
        "\n",
        "    Returns:\n",
        "    PIL.Image: The cropped face from the input image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a copy of the image to draw on\n",
        "    temporary = image.copy()\n",
        "    #plt.imshow(temporary)\n",
        "    # Use the MTCNN model to detect faces in the image\n",
        "    sample = mtcnn.detect(temporary)\n",
        "    print(sample)\n",
        "    # If a face is detected\n",
        "    if sample[0] is not None:\n",
        "        #print(\"IN IFFFF\")\n",
        "        # Get the bounding box coordinates of the face\n",
        "        box = sample[0][0]\n",
        "\n",
        "        # Crop the detected face from the image\n",
        "        face = temporary.crop(box)\n",
        "\n",
        "        # Pre-process the cropped face to be fed into the\n",
        "        # emotion detection model\n",
        "        inputs = extractor(images=face, return_tensors=\"pt\")\n",
        "\n",
        "        # Pass the pre-processed face through the model to\n",
        "        # get emotion predictions\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Apply softmax to the logits to get probabilities\n",
        "        probabilities = torch.nn.functional.softmax(outputs.logits,\n",
        "                                                    dim=-1)\n",
        "\n",
        "        # Retrieve the id2label attribute from the configuration\n",
        "        id2label = AutoConfig.from_pretrained(\n",
        "            \"trpakov/vit-face-expression\", cache_dir = path_cache\n",
        "        ).id2label\n",
        "\n",
        "        # Convert probabilities tensor to a Python list\n",
        "        probabilities = probabilities.detach().numpy().tolist()[0]\n",
        "\n",
        "        # Map class labels to their probabilities\n",
        "        class_probabilities = {id2label[i]: prob for i,\n",
        "                               prob in enumerate(probabilities)}\n",
        "\n",
        "        # Define colors for each emotion\n",
        "        colors = {\n",
        "            \"angry\": \"red\",\n",
        "            \"disgust\": \"green\",\n",
        "            \"fear\": \"gray\",\n",
        "            \"happy\": \"yellow\",\n",
        "            \"neutral\": \"purple\",\n",
        "            \"sad\": \"blue\",\n",
        "            \"surprise\": \"orange\"\n",
        "        }\n",
        "        palette = [colors[label] for label in class_probabilities.keys()]\n",
        "        #print(\"IN IFFF\")\n",
        "        # Prepare a figure with 2 subplots: one for the face image,\n",
        "        # one for the bar plot\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Display the cropped face in the left subplot\n",
        "        axs[0].imshow(np.array(face))\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        # Create a horizontal bar plot of the emotion probabilities in\n",
        "        # the right subplot\n",
        "        sns.barplot(ax=axs[1],\n",
        "                    y=list(class_probabilities.keys()),\n",
        "                    x=[prob * 100 for prob in class_probabilities.values()],\n",
        "                    palette=palette,\n",
        "                    orient='h')\n",
        "        axs[1].set_xlabel('Probability (%)')\n",
        "        axs[1].set_title('Emotion Probabilities')\n",
        "        axs[1].set_xlim([0, 100])  # Set x-axis limits to show percentages\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "        return face, class_probabilities\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHGakH7dsFTf",
        "outputId": "69ac607e-3201-4a6b-fb1f-581a780fc5ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize MTCNN model for single face cropping\n",
        "mtcnn = MTCNN(\n",
        "    image_size=160,\n",
        "    margin=0,\n",
        "    min_face_size=200,\n",
        "    thresholds=[0.6, 0.7, 0.7],\n",
        "    factor=0.709,\n",
        "    post_process=True,\n",
        "    keep_all=False,\n",
        "    device=device\n",
        ")\n",
        "# Load the pre-trained model and feature extractor\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    \"trpakov/vit-face-expression\", cache_dir = path_cache\n",
        ")\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"trpakov/vit-face-expression\", cache_dir = path_cache\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QoCt6WQ-tAuv"
      },
      "outputs": [],
      "source": [
        "def create_combined_image(face, class_probabilities):\n",
        "    \"\"\"\n",
        "    Create an image combining the detected face and a barplot\n",
        "    of the emotion probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    face (PIL.Image): The detected face.\n",
        "    class_probabilities (dict): The probabilities of each\n",
        "        emotion class.\n",
        "\n",
        "    Returns:\n",
        "    np.array: The combined image as a numpy array.\n",
        "    \"\"\"\n",
        "    # Define colors for each emotion\n",
        "    colors = {\n",
        "        \"angry\": \"red\",\n",
        "        \"disgust\": \"green\",\n",
        "        \"fear\": \"gray\",\n",
        "        \"happy\": \"yellow\",\n",
        "        \"neutral\": \"purple\",\n",
        "        \"sad\": \"blue\",\n",
        "        \"surprise\": \"orange\"\n",
        "    }\n",
        "    palette = [colors[label] for label in class_probabilities.keys()]\n",
        "\n",
        "    # Create a figure with 2 subplots: one for the\n",
        "    # face image, one for the barplot\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Display face on the left subplot\n",
        "    axs[0].imshow(np.array(face))\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # Create a barplot of the emotion probabilities\n",
        "    # on the right subplot\n",
        "    sns.barplot(ax=axs[1],\n",
        "                y=list(class_probabilities.keys()),\n",
        "                x=[prob * 100 for prob in class_probabilities.values()],\n",
        "                palette=palette,\n",
        "                orient='h')\n",
        "    axs[1].set_xlabel('Probability (%)')\n",
        "    axs[1].set_title('Emotion Probabilities')\n",
        "    axs[1].set_xlim([0, 100])  # Set x-axis limits\n",
        "\n",
        "    # Convert the figure to a numpy array\n",
        "    canvas = FigureCanvas(fig)\n",
        "    canvas.draw()\n",
        "    img = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
        "    img  = img.reshape(canvas.get_width_height()[::-1] + (3,))\n",
        "\n",
        "    plt.close(fig)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "b-7iP0OLOLL_"
      },
      "outputs": [],
      "source": [
        "# Set cache directories for XDG and Hugging Face Hub\n",
        "os.environ['XDG_CACHE_HOME'] = '/data/mpstme-azeem/msds2023/jlegara/.cache'\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = '/data/mpstme-azeem/msds2023/jlegara/.cache'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cache directories for XDG and Hugging Face Hub\n",
        "os.environ['XDG_CACHE_HOME'] = '/home/msds2023/jlegara/.cache'\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = '/home/msds2023/jlegara/.cache'"
      ],
      "metadata": {
        "id": "Sdg3GKc5Pnj6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_VeAodXOLMA",
        "outputId": "0d6bd1cc-6216-4257-ffbd-76a65875dd52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'daivik_clipped_reaction - Made with Clipchamp.mp4'   sample_data\n",
            " drive\t\t\t\t\t\t      video_chunks\n",
            " final_plot.png\t\t\t\t\t      video_frames\n",
            " models--trpakov--vit-face-expression\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dBfVoQ5Njoke"
      },
      "outputs": [],
      "source": [
        "# Load your video\n",
        "scene = path\n",
        "clip = VideoFileClip(scene)\n",
        "\n",
        "# Save video frames per second\n",
        "vid_fps = clip.fps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyRFtTF_aCho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ad3f02-84fb-4ee7-cdac-ca27c70209c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1021it [05:20, 34.25it/s]"
          ]
        }
      ],
      "source": [
        "# Get the video (as frames)\n",
        "video = clip.without_audio()\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define the batch size and chunk size\n",
        "batch_size = 32  # Number of frames to process at once\n",
        "chunk_size = 500  # Number of frames per file\n",
        "\n",
        "# Temporary list to store a batch of frames\n",
        "batch_frames = []\n",
        "chunk_count = 0  # To keep track of the chunk number\n",
        "\n",
        "# Process video frames\n",
        "for frame in tqdm(video.iter_frames()):\n",
        "    batch_frames.append(np.array(frame))\n",
        "\n",
        "    # When the batch size is reached, store it in the current chunk\n",
        "    if len(batch_frames) >= batch_size:\n",
        "        if len(batch_frames) > 0:\n",
        "            # Save the current batch to the chunk\n",
        "            if 'current_chunk' in locals():\n",
        "                current_chunk.extend(batch_frames)\n",
        "            else:\n",
        "                current_chunk = batch_frames\n",
        "        batch_frames = []  # Clear the temporary batch list to save memory\n",
        "\n",
        "    # If the chunk size is reached, save the chunk to disk and reset\n",
        "    if 'current_chunk' in locals() and len(current_chunk) >= chunk_size:\n",
        "        # Save current chunk as a numpy file\n",
        "        np.save(os.path.join(output_dir, f'video_chunk_{chunk_count}.npy'), np.array(current_chunk))\n",
        "        chunk_count += 1\n",
        "        del current_chunk  # Clear the current chunk from memory\n",
        "\n",
        "# Save any remaining frames in the last chunk\n",
        "if 'current_chunk' in locals() and len(current_chunk) > 0:\n",
        "    np.save(os.path.join(output_dir, f'video_chunk_{chunk_count}.npy'), np.array(current_chunk))\n",
        "\n",
        "# Check how many chunks were created\n",
        "print(f\"Total chunks saved: {chunk_count + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQCcZ53beCSh"
      },
      "outputs": [],
      "source": [
        "# Example of loading a chunk back into memory\n",
        "loaded_chunk = np.load('/data/mpstme-azeem/video_frames/video_chunk_0.npy')\n",
        "print(f\"Loaded chunk shape: {loaded_chunk.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of loading a chunk back into memory\n",
        "loaded_chunk = np.load('/content/video_frames/video_chunk_0.npy')\n",
        "print(f\"Loaded chunk shape: {loaded_chunk.shape}\")"
      ],
      "metadata": {
        "id": "r2fciaYATs05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taHa7iGpfnYI"
      },
      "outputs": [],
      "source": [
        "len(loaded_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJzXfZJUsITG"
      },
      "outputs": [],
      "source": [
        "# Choose a frame\n",
        "frame = loaded_chunk[10]  # choosing the 10th frame\n",
        "\n",
        "# Convert the frame to a PIL image and display it\n",
        "image = Image.fromarray(frame)\n",
        "detect_emotions(image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rest_code(path_chunk, chunk_output_dir, vid_fps):\n",
        "  # Ensure the output directory exists\n",
        "  os.makedirs(chunk_output_dir, exist_ok=True)\n",
        "  skips = 2\n",
        "  reduced_video = []\n",
        "\n",
        "  loaded_chunk = np.load(path_chunk)\n",
        "  print(f\"Loaded chunk shape: {loaded_chunk.shape}\")\n",
        "  # Calculate time per frame in the original video (before skips)\n",
        "  time_per_frame = 1 / vid_fps\n",
        "\n",
        "  for i in tqdm(range(0, len(loaded_chunk), skips)):\n",
        "      reduced_video.append(loaded_chunk[i])\n",
        "\n",
        "  # Define a list of emotions\n",
        "  emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "\n",
        "  # List to hold the combined images\n",
        "  combined_images = []\n",
        "\n",
        "  # Create a list to hold the class probabilities for all frames\n",
        "  all_class_probabilities = []\n",
        "  timestamps = []\n",
        "\n",
        "  # Loop over video frames\n",
        "  for i, frame in tqdm(enumerate(reduced_video),\n",
        "                      total=len(reduced_video),\n",
        "                      desc=\"Processing frames\"):\n",
        "      # Convert frame to uint8\n",
        "      frame = frame.astype(np.uint8)\n",
        "      #print(frame)\n",
        "      # Calculate the accurate timestamp for this frame\n",
        "      # Since we're skipping frames, the timestamp is calculated based on the original frame index (i * skips)\n",
        "      timestamp = (i * skips) * time_per_frame\n",
        "      timestamps.append(timestamp)\n",
        "\n",
        "      # Call detect_emotions to get face and class probabilities\n",
        "      #print(Image.fromarray(frame))\n",
        "      face, class_probabilities = detect_emotions(Image.fromarray(frame))\n",
        "\n",
        "      # If a face was found\n",
        "      if face is not None:\n",
        "          # Create combined image for this frame\n",
        "          combined_image = create_combined_image(face, class_probabilities)\n",
        "\n",
        "          # Append combined image to the list\n",
        "          combined_images.append(combined_image)\n",
        "      else:\n",
        "          # If no face was found, set class probabilities to None\n",
        "          class_probabilities = {emotion: None for emotion in emotions}\n",
        "\n",
        "      # Append class probabilities to the list\n",
        "      all_class_probabilities.append(class_probabilities)\n",
        "\n",
        "  # Convert list of images to video clip\n",
        "  clip_with_plot = ImageSequenceClip(combined_images,\n",
        "                                    fps=vid_fps/skips)  # Choose the frame rate (fps) according to your requirement\n",
        "\n",
        "  # Generate unique filenames for this chunk\n",
        "  chunk_name = os.path.basename(path_chunk).split('.')[0]\n",
        "  video_output_path = os.path.join(chunk_output_dir, f\"{chunk_name}_plot_video.mp4\")\n",
        "  #prob_output_path = os.path.join(chunk_output_dir, f\"{chunk_name}_class_probabilities.pkl\")\n",
        "  df_output_path = os.path.join(chunk_output_dir, f\"{chunk_name}_class_probabilities.csv\")\n",
        "  #reduced_video_output_path = os.path.join(chunk_output_dir, f\"{chunk_name}_reduced_video.npy\")\n",
        "\n",
        "  # Write the video to a file with a specific frame rate\n",
        "  clip_with_plot.write_videofile(video_output_path, fps=vid_fps/skips)\n",
        "\n",
        "  # Display the clip\n",
        "  #clip_with_plot.ipython_display(width=900)\n",
        "  # Save class probabilities as a pickle file\n",
        "  #with open(prob_output_path, 'wb') as f:\n",
        "      #pickle.dump(all_class_probabilities, f)\n",
        "\n",
        "  # Define colors for each emotion\n",
        "  colors = {\n",
        "      \"angry\": \"red\",\n",
        "      \"disgust\": \"green\",\n",
        "      \"fear\": \"gray\",\n",
        "      \"happy\": \"yellow\",\n",
        "      \"neutral\": \"purple\",\n",
        "      \"sad\": \"blue\",\n",
        "      \"surprise\": \"orange\"\n",
        "  }\n",
        "\n",
        "  # Convert list of class probabilities into a DataFrame\n",
        "  df = pd.DataFrame(all_class_probabilities)\n",
        "  df['timestamp'] = timestamps\n",
        "\n",
        "  # Convert probabilities to percentages\n",
        "  df = df * 100\n",
        "  # Save DataFrame to CSV file\n",
        "  df.to_csv(df_output_path, index=False)\n",
        "\n",
        "  # Create a line plot\n",
        "  #plt.figure(figsize=(15,8))\n",
        "  #for emotion in df.columns:\n",
        "      #plt.plot(df[emotion], label=emotion, color=colors[emotion])\n",
        "\n",
        "  #plt.xlabel('Frame Order')\n",
        "  #plt.ylabel('Emotion Probability (%)')\n",
        "  #plt.title('Emotion Probabilities Over Time')\n",
        "  #plt.legend()\n",
        "  #plt.show()\n",
        "  # Save plot to file\n",
        "  #plt.savefig(plot_output_path)\n",
        "  #plt.close()\n",
        "\n",
        "  print(f\"Processed chunk saved: {video_output_path},{df_output_path}\")"
      ],
      "metadata": {
        "id": "IbgBUYzce5jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_dir = \"/content/video_frames/\"\n",
        "chunk_output_dir = os.path.join(\"/content/\", 'video_chunks/')\n",
        "npy_files = [f for f in os.listdir(chunk_dir) if f.endswith('.npy')]\n",
        "npy_files\n",
        "for file in npy_files:\n",
        "  rest_code(os.path.join(chunk_dir, file), chunk_output_dir, vid_fps)"
      ],
      "metadata": {
        "id": "1Xxf42_LvFK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_final_dataframe(chunk_output_dir):\n",
        "    # List all CSV files in the chunk output directory\n",
        "    csv_files = [f for f in os.listdir(chunk_output_dir) if f.endswith('_class_probabilities.csv')]\n",
        "\n",
        "    # Initialize an empty DataFrame to store the final merged DataFrame\n",
        "    final_df = pd.DataFrame()\n",
        "\n",
        "    # Variable to track the current frame count across all chunks\n",
        "    frame_count = 0\n",
        "\n",
        "    # Iterate over each CSV file and concatenate them\n",
        "    for csv_file in sorted(csv_files):\n",
        "        # Load the chunk DataFrame\n",
        "        chunk_df = pd.read_csv(os.path.join(chunk_output_dir, csv_file))\n",
        "\n",
        "        # Update the index to reflect the correct frame order\n",
        "        chunk_df.index = range(frame_count, frame_count + len(chunk_df))\n",
        "\n",
        "        # Append the chunk DataFrame to the final DataFrame\n",
        "        final_df = pd.concat([final_df, chunk_df], axis=0)\n",
        "\n",
        "        # Update the frame count\n",
        "        frame_count += len(chunk_df)\n",
        "\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "qiPslUwee5Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = create_final_dataframe(chunk_output_dir)\n",
        "# Define colors for each emotion\n",
        "colors = {\"angry\": \"red\",\"disgust\": \"green\",\"fear\": \"gray\",\"happy\": \"yellow\",\"neutral\": \"purple\",\"sad\": \"blue\",\"surprise\": \"orange\"}\n",
        "plt.figure(figsize=(15,8))\n",
        "for emotion in final_df.columns:\n",
        "    plt.plot(final_df[emotion], label=emotion, color=colors[emotion])\n",
        "\n",
        "plt.xlabel('Frame Order')\n",
        "plt.ylabel('Emotion Probability (%)')\n",
        "plt.title('Emotion Probabilities Over Time')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.savefig(\"/content/final_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "final_df_output_path = os.path.join(chunk_output_dir, \"final_class_probabilities.csv\")\n",
        "final_df.to_csv(final_df_output_path, index=True)\n",
        "print(f\"Final DataFrame saved to: {final_df_output_path}\")"
      ],
      "metadata": {
        "id": "ush6v_kMe5V7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}